The data set for the assignment can be downloaded from these links:
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-01.csv
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-02.csv
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-03.csv
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-04.csv
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-05.csv
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-06.csv


mv yellow_tripdata_2017-01.csv jan.csv
mv yellow_tripdata_2017-02.csv feb.csv
mv yellow_tripdata_2017-03.csv mar.csv
mv yellow_tripdata_2017-04.csv apr.csv
mv yellow_tripdata_2017-05.csv may.csv
mv yellow_tripdata_2017-06.csv jun.csv



Steps:

1. Create an EMR instance m4.xlarge with 50GB storage and following services:
    Hadoop
    HBase
    Sqoop


Task 1.
2. Create a RDS instance: 
    Database Name: project1db
    User: admin
    Password: project123

3. set up a connection between RDS instance and EC2


4. Connect to RDS from your EMR instance using command:
    mysql -h rds_endpoint -P 3306 -u admin -p

5.  show databases; 
    create database nyctaxi; 
    use nyctaxi;


6. Create table schema in RDS
CREATE TABLE taxi_2017 (
    VendorID              INT, 
    tpep_pickup_datetime  DATETIME, 
    tpep_dropoff_datetime DATETIME, 
    passenger_count       INT, 
    trip_distance         FLOAT, 
    RatecodeID            INT, 
    store_and_fwd_flag    VARCHAR(1), 
    PULocationID          INT, 
    DOLocationID          INT, 
    payment_type          INT, 
    fare_amount           FLOAT, 
    extra                 FLOAT, 
    mta_tax               FLOAT, 
    tip_amount            FLOAT, 
    tolls_amount          FLOAT, 
    improvement_surcharge FLOAT, 
    total_amount          FLOAT, 
    congestion_surcharge  FLOAT, 
    airport_fee           FLOAT 
); 
show tables;


7. Load the data in RDS table from the downloaded data.

    LOAD DATA LOCAL INFILE '/home/hadoop/jan.csv'
    INTO TABLE taxi_2017
    FIELDS TERMINATED BY ','
    LINES TERMINATED BY '\n' 
    IGNORE 1 LINES;

    LOAD DATA LOCAL INFILE '/home/hadoop/feb.csv'
    INTO TABLE taxi_2017
    FIELDS TERMINATED BY ','
    LINES TERMINATED BY '\n' 
    IGNORE 1 LINES;


# Optional : Filling with Nulls for missing values
    LOAD DATA LOCAL INFILE '/home/hadoop/jan.csv'
    INTO TABLE taxi_2017
    FIELDS TERMINATED BY ','
    LINES TERMINATED BY '\n'
    IGNORE 1 LINES
    (VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount,@vcongestion_surcharge,@vairport_fee)
    SET
    congestion_surcharge = NULLIF(@vcongestion_surcharge,''),
    airport_fee = NULLIF(@vairport_fee,'')
    ;
    

# If any warnings in mysql shell, use command  " show warnings; " immediately after running the query that had warnings

8. Add an auto incrementing Primary key to the table to use as row_key in Hbase table later on.
ALTER TABLE taxi_2017
ADD COLUMN id INT AUTO_INCREMENT PRIMARY KEY; 

9. try a few commands to make sure that your tables have indeed been loaded
select * from taxi_2017 limit 5;
select COUNT(*) from taxi_2017;

Validate this count with the original csv file in your instance: 
    wc -l jan.csv
    wc -l feb.csv


--------------------------------------------------------------------------------------------------

Task 2.
10. Ingest data from RDS to HBase using Sqoop

In HBase shell:
    create 'taxidata', {NAME => 'trip_info'},{NAME => 'fare_info'}


- Install mysql connector and setup sqoop : 
sudo -i : first login as root 

wget https://de-mysql-connector.s3.amazonaws.com/mysql-connector-java-8.0.25.tar.gz
tar -xvf mysql-connector-java-8.0.25.tar.gz
cd mysql-connector-java-8.0.25/
sudo cp mysql-connector-java-8.0.25.jar /usr/lib/sqoop/lib/
cp /usr/lib/hbase/hbase-client.jar /usr/lib/sqoop/lib/
cp /usr/lib/hbase/hbase-common.jar /usr/lib/sqoop/lib/


sqoop import \
--connect jdbc:mysql://db3.rds.amazonaws.com:3306/nyctaxi \
--username admin \
--password project123 \
--table taxi_2017 \
--columns "VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,RatecodeID,store_anfwd_flag,PULocationID,DOLocationID,payment_type" \
--hbase-table taxidata \
--column-family trip_info \
--hbase-row-key id \
--hbase-create-table \
--hbase-bulkload


sqoop import \
  --connect jdbc:mysql://db3.rds.amazonaws.com:3306/nyctaxi \
  --username admin \
  --password project123 \
  --table taxi_2017 \
  --columns "fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount,congestion_surcharge,airport_fee" \
  --hbase-table taxidata \
  --column-family fare_info \
  --hbase-row-key id \
  --hbase-bulkload

# Implement null handling accordingly in the sqoop commands

--------------------------------------------------------------------------------------------------------

Task 3. Install Happybase in EMR

sudo -i
sudo yum update
yum install gcc
sudo yum install python3-devel
pip install happybase
jps : it should show that Thriftserver is running else use command :  hbase thrift start


Bulk import data from next two files in the dataset on your EMR cluster to your HBase Table using the relevant happybase codes.
Note: For the above task 3, you just need to import data from the subsequent 2 csv files (i.e. yellow_tripdata_2017-03.csv
& yellow_tripdata_2017-04.csv) on your EMR cluster.

- mkdir hbase
- cd hbase
- mv /home/hadoop/mar.csv .
- mv /home/hadoop/apr.csv .
- vi batch_insert.py
- Run the batch_insert.py in /home/hadoop/hbase

-----------------------------------------------------------------------------------------------------------

Task 4. Install MRJob in EMR : 
- pip install mrjob

Use all locally downloaded files as input
- mv /home/hadoop/* /home/hadoop/mrtasks/input

Use Hadoop runner as file size is large
MapReduce Tasks: task_a to task_f
scp -i ~/my-pair1.pem ~/Hadoop-Project/mapreduce/* hadoop@ec2.amazonaws.com:/home/hadoop
mkdir inputs
mkdir outputs
scp -i ~/my-pair1.pem ~/Hadoop-Project/mapreduce/inputs/* hadoop@ec2.compute-1.amazonaws.com:/home/hadoop/inputs
python mrtask_a.py inputs > outputs/out_a.txt

mv /home/hadoop/* ./inputs

# using hadoop runner
python mrtask_a.py -r hadoop inputs > outputs_h/out_a.txt

# local testing
# python mrtask_a.py inputs > outputs/out_a.txt


Optional:
Try storing the inputs in hdfs then running them using the hadoop runner.

python mrtask_a.py -r hadoop hdfs:///user/root/input > out_a.txt
It takes inputs from hdfs but stores outputs in emr local.

To store both input and output in hdfs:
python mrtask_a.py -r hadoop hdfs:///user/root/input  --output-dir=hdfs:///user/root/outputs/


------------------------------------------------------------------------------------------------------------

# optional dashboard task
Export results of mapreduce tasks from hdfs to rds : Do this for mr tasks c,d,e,f

first create a database and all the necessary tables where the outputs will be exported.

sqoop export \
  --connect jdbc:mysql://<RDS_ENDPOINT>:<RDS_PORT>/<DB_NAME> \
  --username <RDS_USERNAME> \
  --password <RDS_PASSWORD> \
  --table <TABLE_NAME> \
  --export-dir <HDFS_DIRECTORY_PATH> \
  --input-fields-terminated-by '\t'

sqoop export \
  --connect jdbc:mysql://database-1.cdnxpt5g8vv4.us-east-1.rds.amazonaws.com:3306/results \
  --username admin \
  --password project123 \
  --table out_a \
  --export-dir /user/root/outputs/out_a \
  --input-fields-terminated-by '\t'