The data set for the assignment can be downloaded from these links:
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-01.csv
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-02.csv
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-03.csv
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-04.csv
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-05.csv
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-06.csv

mv yellow_tripdata_2017-01.csv jan.csv
mv yellow_tripdata_2017-02.csv feb.csv
mv yellow_tripdata_2017-03.csv mar.csv
mv yellow_tripdata_2017-04.csv apr.csv
mv yellow_tripdata_2017-05.csv may.csv
mv yellow_tripdata_2017-06.csv jun.csv

input data is for the year 2017.



Steps:

1. Create an EMR instance m4.xlarge with 50GB storage and following services:
    Hadoop
    HBase
    Sqoop



Task 1.
2. Create a RDS instance:
    Database Name: project1db
    User: admin
    Password: project123

3. set up a connection between RDS instance and EC2

4. Download the dataset in EMR
- mv yellow_tripdata_2017-01.csv jan.csv
- mv yellow_tripdata_2017-02.csv feb.csv

5. Connect to RDS from your EMR instance using command:
    mysql -h demodb.cqsesz6h9yjg.us-east-1.rds.amazonaws.com -P 3306 -u admin -p

6.  show databases; 
    create database nyctaxi; 
    use nyctaxi;

7. Create table schema in RDS
CREATE TABLE taxi_2017 
(   id                    INT auto_increment PRIMARY KEY, 
    vendorid              INT, 
    pickup_time           DATETIME, 
    dropoff_time          DATETIME, 
    passenger_count       INT, 
    trip_distance         FLOAT, 
    ratecodeid            INT, 
    store_and_fwd_flag    VARCHAR(1), 
    pulocationid          INT, 
    dolocationid          INT, 
    payment_type          INT, 
    fare_amount           FLOAT, 
    extra                 FLOAT, 
    mta_tax               FLOAT, 
    tip_amount            FLOAT, 
    tolls_amount          FLOAT, 
    improvement_surcharge FLOAT, 
    total_amount          FLOAT, 
    congestion_surcharge  FLOAT, 
    airport_fee           FLOAT 
); 
show tables;


8. Load the data in RDS table from the downloaded data.

    LOAD DATA LOCAL INFILE '/home/hadoop/jan.csv'
    INTO TABLE taxi_2017
    FIELDS TERMINATED BY ','
    LINES TERMINATED BY '\n' 
    IGNORE 1 LINES;

do it also for feb.csv
# If any warnings in mysql shell, use command  " show warnings; " immediately after running the query that had warnings


9. try a few commands to make sure that your tables have indeed been loaded
select * from taxi_2017 limit 5;
select COUNT(*) from taxi_2017;
Validate this count with the original csv file in your instance: 
    wc -l jan.csv
    wc -l feb.csv




Task 2.
10. Ingest data from RDS to HBase using Sqoop
In HBase shell:
    CREATE 'taxi_data', {NAME => 'trip_info'},{NAME => 'fare_info'}

    trip_info : (11 cols)
        id int Primary key,
        VendorID int,
        pickup_time DATETIME,
        dropoff_time DATETIME,
        passenger_count int,
        trip_distance float,
        RateCodeID int,
        store_and_fwd_flag varchar(1),
        PULocationID int,
        DOLocationID int,
        payment_type int
        
    fare_info : (9 cols)
        fare_amount float,
        extra float,
        mta_tax float,
        tip_amount float, 
        tolls_amount float, 
        improvement_surcharge float, 
        total_amount float, 
        congestion_surcharge float,
        airport_fee float

sqoop import \
--connect jdbc:mysql://your_rds_endpoint:your_rds_port/nyctaxi \
--username admin \
--password project123 \
--table taxi_2017 \
--columns "VendorID,pickup_time,dropoff_time,passenger_count,trip_distance,RateCodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type" \
--hbase-table taxi_data \
--column-family trip_info \
--hbase-row-key id \
--hbase-create-table \
--hbase-bulkload


sqoop import \
  --connect jdbc:mysql://your_rds_endpoint:your_rds_port/nyctaxi \
  --username admin \
  --password project123 \
  --table taxi_2017 \
  --columns "fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, airport_fee" \
  --hbase-table taxi_data \
  --column-family fare_info \
  --hbase-row-key id \
  --hbase-bulkload

# Implement null handling accordingly in the sqoop commands



Task 3. Install Happybase in EMR

Bulk import data from next two files in the dataset on your EMR cluster to your HBase Table using the relevant happybase codes.
Note: For the above task 3, you just need to import data from the subsequent 2 csv files (i.e. yellow_tripdata_2017-03.csv
& yellow_tripdata_2017-04.csv) on your EMR cluster.

- mkdir hbase
- cd hbase
- mv /home/hadoop/mar.csv .
- mv /home/hadoop/apr.csv .
- vi batch_insert.py
- Run the batch_insert.py in /home/hadoop/hbase



Task 4. Install MRJob in EMR : 
- pip install mrjob

Use all locally downloaded files as input
- mv /home/hadoop/* /home/hadoop/mrtasks/input

Use Hadoop runner as file size is large
MapReduce Tasks: task_a to task_f





Submission:
Upload a ZIP file containing the following:

Document-01: A PDF document (RDS.pdf) containing the codes, with the explanations, used for 
loading the datasets mentioned into an AWS RDS instance. This should have the code along with 
the screenshots of the EMR instance showing the table creation.

Document-02: A PDF document (Ingestiontask.pdf) containing the code to create the HBase table. 
The file should also include the Sqoop command to ingest data from RDS into the HBase table. 
The document should be well commented explaining the code.

The Python code (batch_ingest.py) used to ingest the batch data to the HBase table.

The Python codes used for the MapReduce tasks. The files should be labelled as mrtask_a.py, 
mrtask_b.py, and so on based on the relevant question number. This scripts should also contain the 
relevant and proper comments, explaining all the steps taken. 
Answers to the query and the screenshots of the results of the MapReduce tasks must be 
included in a separate document (MapReducetasks.pdf) in sequence.


# optional dashboard task
Export results of mapreduce tasks from hdfs to rds : Do this for mr tasks c,d,e,f

sqoop export \
  --connect jdbc:mysql://<RDS_ENDPOINT>:<RDS_PORT>/<DB_NAME> \
  --username <RDS_USERNAME> \
  --password <RDS_PASSWORD> \
  --table <TABLE_NAME> \
  --export-dir <HDFS_DIRECTORY_PATH> \
  --input-fields-terminated-by '\t'