The data set for the assignment can be downloaded from these links:
https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-01.csv
https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-02.csv
https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-03.csv
https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-04.csv
https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-05.csv
https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-06.csv

input data is for the year 2017.



Steps:

1. Create an EMR instance m4.xlarge with 50GB storage and following services:
    Hadoop
    HBase
    Sqoop

Task 1.
2. Create a RDS instance:
    Database Name: project1db
    User: admin
    Password: project123

3. set up a connection between RDS instance and EC2

4. Connect to RDS from your EMR instance using command:
    mysql -h demodb.cqsesz6h9yjg.us-east-1.rds.amazonaws.com -P 3306 -u admin -p

5.  show databases; 
    create database nyctaxi; 
    use nyctaxi;

6. Create table schema in RDS
CREATE TABLE taxi_2017 
(   id                    INT auto_increment PRIMARY KEY, 
    vendorid              INT, 
    pickup_time           DATETIME, 
    dropoff_time          DATETIME, 
    passenger_count       INT, 
    trip_distance         FLOAT, 
    ratecodeid            INT, 
    store_and_fwd_flag    VARCHAR(1), 
    pulocationid          INT, 
    dolocationid          INT, 
    payment_type          INT, 
    fare_amount           FLOAT, 
    extra                 FLOAT, 
    mta_tax               FLOAT, 
    tip_amount            FLOAT, 
    tolls_amount          FLOAT, 
    improvement_surcharge FLOAT, 
    total_amount          FLOAT, 
    congestion_surcharge  FLOAT, 
    airport_fee           FLOAT 
); 
show tables;

7. Download the dataset in EMR
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-01.csv
wget https://nyc-tlc-upgrad.s3.amazonaws.com/yellow_tripdata_2017-02.csv

8. Connect to RDS : mysql -h demodb.cqsesz6h9yjg.us-east-1.rds.amazonaws.com -P 3306 -u admin -p

9. Load the data in RDS table from the downloaded data.
    use nyctaxi;

    LOAD DATA LOCAL INFILE '/home/hadoop/yellow_tripdata_2017-01.csv'
    INTO TABLE taxi_2017
    FIELDS TERMINATED BY ','
    LINES TERMINATED BY '\n' 
    IGNORE 1 LINES;

do it also for yellow_tripdata_2017-02.csv

# If any warnings in mysql shell, use command  " show warnings; " immediately after running the query that had warnings

10. try a few commands to make sure that your tables have indeed been loaded
select * from taxi_2017 limit 5;
select COUNT(*) from taxi_2017;
Validate this count with the original csv file in your instance: 
    wc -l <file.csv>


Task 2.
11. Ingest data from RDS to HBase using Sqoop
In HBase shell:
    CREATE 'taxi_data', {NAME => 'trip_info'},{NAME => 'fare_info'}

    trip_info : (11 cols)
        id int Primary key,
        VendorID int,
        pickup_time DATETIME,
        dropoff_time DATETIME,
        passenger_count int,
        trip_distance float,
        RateCodeID int,
        store_and_fwd_flag varchar(1),
        PULocationID int,
        DOLocationID int,
        payment_type int
        
    fare_info : (9 cols)
        fare_amount float,
        extra float,
        mta_tax float,
        tip_amount float, 
        tolls_amount float, 
        improvement_surcharge float, 
        total_amount float, 
        congestion_surcharge float,
        airport_fee float

sqoop import \
--connect jdbc:mysql://your_rds_endpoint:your_rds_port/nyctaxi \
--username admin \
--password project123 \
--table taxi_2017 \
--columns "VendorID,pickup_time,dropoff_time,passenger_count,trip_distance,RateCodeID,store_and_fwd_flag,PULocationID,DOLocationID,payment_type" \
--hbase-table taxi_data \
--column-family trip_info \
--hbase-row-key id \
--hbase-create-table \
--hbase-bulkload


sqoop import \
  --connect jdbc:mysql://your_rds_endpoint:your_rds_port/nyctaxi \
  --username admin \
  --password project123 \
  --table taxi_2017 \
  --columns "fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge, airport_fee" \
  --hbase-table taxi_data \
  --column-family fare_info \
  --hbase-row-key id \
  --hbase-bulkload

# Implement null handling accordingly in the sqoop commands


Task 3. Install Happybase in EMR
Bulk import data from next two files in the dataset on your EMR cluster to your HBase Table using the relevant happybase codes.
Note: For the above task 3, you just need to import data from the subsequent 2 csv files (i.e. yellow_tripdata_2017-03.csv
& yellow_tripdata_2017-04.csv) on your EMR cluster.


Task 4. Install MRJob in EMR : pip install mrjob
Use Hadoop runner as file size is large
MapReduce Tasks: task_a to task_f
Use all locally downloaded files as input


Upload a ZIP file containing the following:

Document-01: A PDF document (RDS.pdf) containing the codes, with the explanations, used for 
loading the datasets mentioned into an AWS RDS instance. This should have the code along with 
the screenshots of the EMR instance showing the table creation.

Document-02: A PDF document (Ingestiontask.pdf) containing the code to create the HBase table. 
The file should also include the Sqoop command to ingest data from RDS into the HBase table. 
The document should be well commented explaining the code.

The Python code (batch_ingest.py) used to ingest the batch data to the HBase table.

The Python codes used for the MapReduce tasks. The files should be labelled as mrtask_a.py, 
mrtask_b.py, and so on based on the relevant question number. This scripts should also contain the 
relevant and proper comments, explaining all the steps taken. 
Answers to the query and the screenshots of the results of the MapReduce tasks must be 
included in a separate document (MapReducetasks.pdf) in sequence.


# optional dashboard task
Export results of mapreduce tasks from hdfs to rds : Do this for mr tasks c,d,e,f

sqoop export \
  --connect jdbc:mysql://<RDS_ENDPOINT>:<RDS_PORT>/<DB_NAME> \
  --username <RDS_USERNAME> \
  --password <RDS_PASSWORD> \
  --table <TABLE_NAME> \
  --export-dir <HDFS_DIRECTORY_PATH> \
  --input-fields-terminated-by '\t'